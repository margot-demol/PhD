Warning: no access to tty (Bad file descriptor).
Thus no job control in this shell.
Mon Jun 27 11:30:57 GMT 2022
jlab.py:30: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  elif dashinfo is not '0':
jlab.py:33: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if dashinfo is not '0':
Fail to get yarn configuration. {"type":"error","data":"An unexpected error occurred: \"ENOENT: no such file or directory, open '/home1/.yarnrc'\"."}
{"type":"info","data":"If you think this is a bug, please open a bug report with the information provided in \"/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/jupyterlab/yarn-error.log\"."}
{"type":"info","data":"Visit https://yarnpkg.com/en/docs/cli/config for documentation about this command."}

[I 2022-06-27 11:31:16.573 ServerApp] jupyterlab | extension was successfully linked.
[W 2022-06-27 11:31:16.577 NotebookApp] 'password' has moved from NotebookApp to ServerApp. This config will be passed to ServerApp. Be sure to update your config before our next release.
[I 2022-06-27 11:31:16.584 ServerApp] nbclassic | extension was successfully linked.
[I 2022-06-27 11:31:22.143 ServerApp] notebook_shim | extension was successfully linked.
[I 2022-06-27 11:31:22.983 ServerApp] notebook_shim | extension was successfully loaded.
[I 2022-06-27 11:31:22.984 LabApp] JupyterLab extension loaded from /home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/jupyterlab
[I 2022-06-27 11:31:22.984 LabApp] JupyterLab application directory is /home1/datahome/mdemol/.miniconda3/envs/m2env/share/jupyter/lab
[I 2022-06-27 11:31:22.988 ServerApp] jupyterlab | extension was successfully loaded.
[I 2022-06-27 11:31:23.129 ServerApp] nbclassic | extension was successfully loaded.
[I 2022-06-27 11:31:23.130 ServerApp] Serving notebooks from local directory: /home1/datahome/mdemol
[I 2022-06-27 11:31:23.130 ServerApp] Jupyter Server 1.16.0 is running at:
[I 2022-06-27 11:31:23.130 ServerApp] http://r2i0n17:8877/lab
[I 2022-06-27 11:31:23.130 ServerApp]  or http://127.0.0.1:8877/lab
[I 2022-06-27 11:31:23.130 ServerApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[W 2022-06-27 11:33:24.377 ServerApp] 404 GET /apple-touch-icon-precomposed.png (10.148.1.145) 177.26ms referer=None
[W 2022-06-27 11:33:25.299 ServerApp] 404 GET /apple-touch-icon.png (10.148.1.145) 1.03ms referer=None
[I 2022-06-27 11:33:27.862 LabApp] Build is up to date
[I 2022-06-27 11:33:34.536 ServerApp] Kernel started: d148c5ff-bd99-4645-a01c-068f0b87a957
[I 2022-06-27 11:33:34.537 ServerApp] Kernel started: e5c4908a-ae6c-490e-aa89-bc05b760b665
[I 2022-06-27 11:33:34.538 ServerApp] Kernel started: 70a62b0f-7d51-4e5e-aaf3-645400b3fabf
[I 2022-06-27 11:33:34.538 ServerApp] Kernel started: 9ba44625-cddc-4d31-8a94-b19fcb3506f2
[I 2022-06-27 11:33:34.712 ServerApp] Kernel started: 6b4a0f21-8857-420c-aa08-a8a68a57fc60
[I 2022-06-27 11:33:34.941 ServerApp] Kernel started: 4bb49fd4-7fa7-4b7e-95d4-35d8ae29d887
[I 2022-06-27 11:33:34.950 ServerApp] Kernel started: 15e070c9-ce07-4d5e-98cb-4850a72e84f5
Traceback (most recent call last):
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/ipykernel_launcher.py", line 15, in <module>
    from ipykernel import kernelapp as app
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/ipykernel/__init__.py", line 5, in <module>
    from .connect import *  # noqa
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/ipykernel/connect.py", line 10, in <module>
    import jupyter_client
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/jupyter_client/__init__.py", line 6, in <module>
    from .asynchronous import AsyncKernelClient  # noqa
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/jupyter_client/asynchronous/__init__.py", line 1, in <module>
    from .client import AsyncKernelClient  # noqa
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/jupyter_client/asynchronous/client.py", line 6, in <module>
    from jupyter_client.channels import HBChannel
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/jupyter_client/channels.py", line 15, in <module>
    from .session import Session
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/jupyter_client/session.py", line 52, in <module>
    from jupyter_client.jsonutil import extract_dates
  File "<frozen importlib._bootstrap>", line 991, in _find_and_load
  File "<frozen importlib._bootstrap>", line 975, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 671, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 839, in exec_module
  File "<frozen importlib._bootstrap_external>", line 934, in get_code
  File "<frozen importlib._bootstrap_external>", line 1032, in get_data
KeyboardInterrupt
[I 2022-06-27 11:33:42.253 ServerApp] Kernel restarted: 6b4a0f21-8857-420c-aa08-a8a68a57fc60
readline: /etc/inputrc: line 19: term: unknown variable name
readline: /etc/inputrc: line 19: term: unknown variable name
[IPKernelApp] ERROR | No such comm target registered: jupyter.widget.control
[IPKernelApp] WARNING | No such comm: 2ad959cf-fe66-473c-8119-a41b9f934a76
[IPKernelApp] ERROR | No such comm target registered: jupyter.widget.control
[IPKernelApp] WARNING | No such comm: 121951cd-f7ac-4dd9-86d6-e6458bbe543e
readline: /etc/inputrc: line 19: term: unknown variable name
[IPKernelApp] ERROR | No such comm target registered: jupyter.widget.control
[IPKernelApp] WARNING | No such comm: c0a84c8a-2438-46dd-913d-3299bf8ba030
readline: /etc/inputrc: line 19: term: unknown variable name
[IPKernelApp] ERROR | No such comm target registered: jupyter.widget.control
[IPKernelApp] WARNING | No such comm: 2a3d92bd-2ed8-422b-ab4e-50c7370a3254
readline: /etc/inputrc: line 19: term: unknown variable name
[W 2022-06-27 11:34:12.887 ServerApp] Replacing stale connection: 6b4a0f21-8857-420c-aa08-a8a68a57fc60:d397cb51-842b-4f78-9377-68350700b47e
[IPKernelApp] ERROR | No such comm target registered: jupyter.widget.control
[IPKernelApp] WARNING | No such comm: b8cd4849-bab0-4eed-ab10-18c92b45c229
readline: /etc/inputrc: line 19: term: unknown variable name
readline: /etc/inputrc: line 19: term: unknown variable name
[IPKernelApp] ERROR | No such comm target registered: jupyter.widget.control
[IPKernelApp] WARNING | No such comm: e8e58c52-a443-424b-8f2b-8366d4385163
[I 2022-06-27 11:35:32.864 ServerApp] Saving file at /m2_2022/sandbox/diag_std_ind.ipynb
[W 2022-06-27 11:46:12.352 ServerApp] WebSocket ping timeout after 119997 ms.
[W 2022-06-27 11:46:12.390 ServerApp] WebSocket ping timeout after 119999 ms.
[W 2022-06-27 11:46:12.498 ServerApp] WebSocket ping timeout after 119998 ms.
[W 2022-06-27 11:46:12.654 ServerApp] WebSocket ping timeout after 119997 ms.
[W 2022-06-27 11:46:12.881 ServerApp] WebSocket ping timeout after 119997 ms.
[W 2022-06-27 11:46:13.028 ServerApp] WebSocket ping timeout after 119998 ms.
[W 2022-06-27 11:46:13.029 ServerApp] WebSocket ping timeout after 119998 ms.
[W 2022-06-27 11:46:13.103 ServerApp] WebSocket ping timeout after 119998 ms.
[W 2022-06-27 11:46:13.113 ServerApp] WebSocket ping timeout after 119999 ms.
[I 2022-06-27 11:46:17.355 ServerApp] Starting buffering for d148c5ff-bd99-4645-a01c-068f0b87a957:f2e55743-7b22-4d69-8a2e-169f0334ce23
[I 2022-06-27 11:46:17.391 ServerApp] Starting buffering for 9ba44625-cddc-4d31-8a94-b19fcb3506f2:7a352307-1440-4e5d-b9aa-916a95f28c9b
[I 2022-06-27 11:46:17.499 ServerApp] Starting buffering for 70a62b0f-7d51-4e5e-aaf3-645400b3fabf:9bb9821b-e2d8-439b-b244-f823e8be1132
[I 2022-06-27 11:46:17.655 ServerApp] Starting buffering for e5c4908a-ae6c-490e-aa89-bc05b760b665:af095eda-c55f-43b5-973f-16c9a28aa17d
[I 2022-06-27 11:46:17.881 ServerApp] Starting buffering for 4bb49fd4-7fa7-4b7e-95d4-35d8ae29d887:c7704ede-c73a-42bd-b91f-d4889d7ba706
[I 2022-06-27 11:46:18.030 ServerApp] Starting buffering for 15e070c9-ce07-4d5e-98cb-4850a72e84f5:e305d13f-0e2f-412b-afe9-db5540622241
[I 2022-06-27 11:46:18.114 ServerApp] Starting buffering for 6b4a0f21-8857-420c-aa08-a8a68a57fc60:d397cb51-842b-4f78-9377-68350700b47e
[I 2022-06-27 11:50:12.116 ServerApp] Restoring connection for d148c5ff-bd99-4645-a01c-068f0b87a957:f2e55743-7b22-4d69-8a2e-169f0334ce23
[I 2022-06-27 11:50:12.125 ServerApp] Restoring connection for 70a62b0f-7d51-4e5e-aaf3-645400b3fabf:9bb9821b-e2d8-439b-b244-f823e8be1132
[I 2022-06-27 11:50:12.125 ServerApp] Restoring connection for 9ba44625-cddc-4d31-8a94-b19fcb3506f2:7a352307-1440-4e5d-b9aa-916a95f28c9b
[I 2022-06-27 11:50:12.128 ServerApp] Restoring connection for e5c4908a-ae6c-490e-aa89-bc05b760b665:af095eda-c55f-43b5-973f-16c9a28aa17d
[I 2022-06-27 11:50:12.128 ServerApp] Restoring connection for 4bb49fd4-7fa7-4b7e-95d4-35d8ae29d887:c7704ede-c73a-42bd-b91f-d4889d7ba706
[IPKernelApp] ERROR | No such comm target registered: jupyter.widget.control
[IPKernelApp] WARNING | No such comm: d3f5ccc3-bd1b-44a9-ad20-494ef46bc9a6
[IPKernelApp] ERROR | No such comm target registered: jupyter.widget.control
[IPKernelApp] ERROR | No such comm target registered: jupyter.widget.control
[IPKernelApp] WARNING | No such comm: 7e056f73-03c5-4578-9c9b-4ac7c164cc25
[IPKernelApp] ERROR | No such comm target registered: jupyter.widget.control
[IPKernelApp] WARNING | No such comm: da036ab8-97f8-4617-9a0d-a8d3b9f6860b
[IPKernelApp] WARNING | No such comm: 54807251-5b66-4e94-ade9-d3393a63c7c4
[IPKernelApp] ERROR | No such comm target registered: jupyter.widget.control
[IPKernelApp] WARNING | No such comm: 2177535e-703e-4142-ae0a-950f5b29758f
[IPKernelApp] ERROR | No such comm target registered: jupyter.widget.control
[IPKernelApp] WARNING | No such comm: f37fdec7-72f5-4051-840c-09cd64caa66d
[I 2022-06-27 11:50:45.738 ServerApp] Saving file at /m2_2022/sandbox/diag_std_ind.ipynb
[I 2022-06-27 12:01:53.427 ServerApp] Kernel interrupted: 6b4a0f21-8857-420c-aa08-a8a68a57fc60
[I 2022-06-27 12:01:54.000 ServerApp] Kernel interrupted: 6b4a0f21-8857-420c-aa08-a8a68a57fc60
[I 2022-06-27 12:02:01.341 ServerApp] Kernel interrupted: 6b4a0f21-8857-420c-aa08-a8a68a57fc60
[I 2022-06-27 12:02:01.550 ServerApp] Kernel interrupted: 6b4a0f21-8857-420c-aa08-a8a68a57fc60
[I 2022-06-27 12:02:01.745 ServerApp] Kernel interrupted: 6b4a0f21-8857-420c-aa08-a8a68a57fc60
[I 2022-06-27 12:02:07.459 ServerApp] Kernel restarted: 6b4a0f21-8857-420c-aa08-a8a68a57fc60
readline: /etc/inputrc: line 19: term: unknown variable name
distributed.diskutils - INFO - Found stale lock file and directory '/home1/datahome/mdemol/m2_2022/sandbox/dask-worker-space/worker-7ommwwn2', purging
distributed.diskutils - INFO - Found stale lock file and directory '/home1/datahome/mdemol/m2_2022/sandbox/dask-worker-space/worker-908pb7j1', purging
distributed.diskutils - INFO - Found stale lock file and directory '/home1/datahome/mdemol/m2_2022/sandbox/dask-worker-space/worker-25_sdtd3', purging
distributed.diskutils - INFO - Found stale lock file and directory '/home1/datahome/mdemol/m2_2022/sandbox/dask-worker-space/worker-8qf5hjfo', purging
distributed.diskutils - INFO - Found stale lock file and directory '/home1/datahome/mdemol/m2_2022/sandbox/dask-worker-space/worker-4wmlfzd4', purging
distributed.diskutils - INFO - Found stale lock file and directory '/home1/datahome/mdemol/m2_2022/sandbox/dask-worker-space/worker-1vreuqx3', purging
distributed.diskutils - INFO - Found stale lock file and directory '/home1/datahome/mdemol/m2_2022/sandbox/dask-worker-space/worker-urtp53jj', purging
distributed.diskutils - INFO - Found stale lock file and directory '/home1/datahome/mdemol/m2_2022/sandbox/dask-worker-space/worker-g17uf2rs', purging
[I 2022-06-27 12:02:45.963 ServerApp] Saving file at /m2_2022/sandbox/diag_std_ind.ipynb
/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:49429
Traceback (most recent call last):
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/asyncio/tasks.py", line 465, in wait_for
    fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/comm/core.py", line 327, in connect
    await asyncio.wait_for(comm.write(local_info), time_left())
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/asyncio/tasks.py", line 467, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/worker.py", line 2949, in gather_dep
    response = await get_data_from_worker(
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/worker.py", line 4173, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/utils_comm.py", line 386, in retry_operation
    return await retry(
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/utils_comm.py", line 371, in retry
    return await coro()
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/worker.py", line 4150, in _get_data
    comm = await rpc.connect(worker)
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/core.py", line 1146, in connect
    return await connect_attempt
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/core.py", line 1082, in _connect
    comm = await connect(
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/comm/core.py", line 331, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://127.0.0.1:49429 after 10 s
distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:52803
Traceback (most recent call last):
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/asyncio/tasks.py", line 465, in wait_for
    fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/comm/core.py", line 327, in connect
    await asyncio.wait_for(comm.write(local_info), time_left())
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/asyncio/tasks.py", line 467, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/worker.py", line 2949, in gather_dep
    response = await get_data_from_worker(
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/worker.py", line 4173, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/utils_comm.py", line 386, in retry_operation
    return await retry(
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/utils_comm.py", line 371, in retry
    return await coro()
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/worker.py", line 4150, in _get_data
    comm = await rpc.connect(worker)
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/core.py", line 1146, in connect
    return await connect_attempt
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/core.py", line 1082, in _connect
    comm = await connect(
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/comm/core.py", line 331, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://127.0.0.1:52803 after 10 s
/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:49429
Traceback (most recent call last):
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/asyncio/tasks.py", line 465, in wait_for
    fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/comm/core.py", line 327, in connect
    await asyncio.wait_for(comm.write(local_info), time_left())
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/asyncio/tasks.py", line 467, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/worker.py", line 2949, in gather_dep
    response = await get_data_from_worker(
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/worker.py", line 4173, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/utils_comm.py", line 386, in retry_operation
    return await retry(
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/utils_comm.py", line 371, in retry
    return await coro()
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/worker.py", line 4150, in _get_data
    comm = await rpc.connect(worker)
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/core.py", line 1146, in connect
    return await connect_attempt
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/core.py", line 1082, in _connect
    comm = await connect(
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/comm/core.py", line 331, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://127.0.0.1:49429 after 10 s
distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:52803
Traceback (most recent call last):
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/asyncio/tasks.py", line 465, in wait_for
    fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/comm/core.py", line 327, in connect
    await asyncio.wait_for(comm.write(local_info), time_left())
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/asyncio/tasks.py", line 467, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/worker.py", line 2949, in gather_dep
    response = await get_data_from_worker(
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/worker.py", line 4173, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/utils_comm.py", line 386, in retry_operation
    return await retry(
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/utils_comm.py", line 371, in retry
    return await coro()
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/worker.py", line 4150, in _get_data
    comm = await rpc.connect(worker)
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/core.py", line 1146, in connect
    return await connect_attempt
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/core.py", line 1082, in _connect
    comm = await connect(
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/comm/core.py", line 331, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://127.0.0.1:52803 after 10 s
/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:49429
Traceback (most recent call last):
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/asyncio/tasks.py", line 465, in wait_for
    fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/comm/core.py", line 327, in connect
    await asyncio.wait_for(comm.write(local_info), time_left())
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/asyncio/tasks.py", line 467, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/worker.py", line 2949, in gather_dep
    response = await get_data_from_worker(
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/worker.py", line 4173, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/utils_comm.py", line 386, in retry_operation
    return await retry(
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/utils_comm.py", line 371, in retry
    return await coro()
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/worker.py", line 4150, in _get_data
    comm = await rpc.connect(worker)
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/core.py", line 1146, in connect
    return await connect_attempt
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/core.py", line 1082, in _connect
    comm = await connect(
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/comm/core.py", line 331, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://127.0.0.1:49429 after 10 s
distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:52803
Traceback (most recent call last):
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/asyncio/tasks.py", line 465, in wait_for
    fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/comm/core.py", line 327, in connect
    await asyncio.wait_for(comm.write(local_info), time_left())
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/asyncio/tasks.py", line 467, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/worker.py", line 2949, in gather_dep
    response = await get_data_from_worker(
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/worker.py", line 4173, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/utils_comm.py", line 386, in retry_operation
    return await retry(
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/utils_comm.py", line 371, in retry
    return await coro()
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/worker.py", line 4150, in _get_data
    comm = await rpc.connect(worker)
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/core.py", line 1146, in connect
    return await connect_attempt
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/core.py", line 1082, in _connect
    comm = await connect(
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/comm/core.py", line 331, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://127.0.0.1:52803 after 10 s
distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:49429
Traceback (most recent call last):
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/asyncio/tasks.py", line 465, in wait_for
    fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/comm/core.py", line 327, in connect
    await asyncio.wait_for(comm.write(local_info), time_left())
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/asyncio/tasks.py", line 467, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/worker.py", line 2949, in gather_dep
    response = await get_data_from_worker(
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/worker.py", line 4173, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/utils_comm.py", line 386, in retry_operation
    return await retry(
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/utils_comm.py", line 371, in retry
    return await coro()
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/worker.py", line 4150, in _get_data
    comm = await rpc.connect(worker)
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/core.py", line 1146, in connect
    return await connect_attempt
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/core.py", line 1082, in _connect
    comm = await connect(
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/comm/core.py", line 331, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://127.0.0.1:49429 after 10 s
distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:52803
Traceback (most recent call last):
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/asyncio/tasks.py", line 465, in wait_for
    fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/comm/core.py", line 327, in connect
    await asyncio.wait_for(comm.write(local_info), time_left())
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/asyncio/tasks.py", line 467, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/worker.py", line 2949, in gather_dep
    response = await get_data_from_worker(
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/worker.py", line 4173, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/utils_comm.py", line 386, in retry_operation
    return await retry(
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/utils_comm.py", line 371, in retry
    return await coro()
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/worker.py", line 4150, in _get_data
    comm = await rpc.connect(worker)
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/core.py", line 1146, in connect
    return await connect_attempt
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/core.py", line 1082, in _connect
    comm = await connect(
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/comm/core.py", line 331, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://127.0.0.1:52803 after 10 s
[I 2022-06-27 12:04:46.154 ServerApp] Saving file at /m2_2022/sandbox/diag_std_ind.ipynb
[I 2022-06-27 12:06:17.613 ServerApp] Saving file at /m2_2022/m2lib22/stress_to_windterm.py
[I 2022-06-27 12:06:39.074 ServerApp] Saving file at /m2_2022/m2lib22/stress_to_windterm.py
[I 2022-06-27 12:06:58.791 ServerApp] Kernel restarted: 6b4a0f21-8857-420c-aa08-a8a68a57fc60
readline: /etc/inputrc: line 19: term: unknown variable name
[I 2022-06-27 12:08:46.330 ServerApp] Saving file at /m2_2022/sandbox/diag_std_ind.ipynb
[I 2022-06-27 12:10:31.681 ServerApp] Saving file at /m2_2022/m2lib22/stress_to_windterm.py
/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
[I 2022-06-27 12:10:46.590 ServerApp] Saving file at /m2_2022/sandbox/diag_std_ind.ipynb
/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:43289
Traceback (most recent call last):
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/asyncio/tasks.py", line 465, in wait_for
    fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/comm/core.py", line 327, in connect
    await asyncio.wait_for(comm.write(local_info), time_left())
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/asyncio/tasks.py", line 467, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/worker.py", line 2949, in gather_dep
    response = await get_data_from_worker(
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/worker.py", line 4173, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/utils_comm.py", line 386, in retry_operation
    return await retry(
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/utils_comm.py", line 371, in retry
    return await coro()
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/worker.py", line 4150, in _get_data
    comm = await rpc.connect(worker)
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/core.py", line 1146, in connect
    return await connect_attempt
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/core.py", line 1082, in _connect
    comm = await connect(
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/comm/core.py", line 331, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://127.0.0.1:43289 after 10 s
distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:44567
Traceback (most recent call last):
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/asyncio/tasks.py", line 465, in wait_for
    fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/comm/core.py", line 327, in connect
    await asyncio.wait_for(comm.write(local_info), time_left())
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/asyncio/tasks.py", line 467, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/worker.py", line 2949, in gather_dep
    response = await get_data_from_worker(
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/worker.py", line 4173, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/utils_comm.py", line 386, in retry_operation
    return await retry(
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/utils_comm.py", line 371, in retry
    return await coro()
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/worker.py", line 4150, in _get_data
    comm = await rpc.connect(worker)
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/core.py", line 1146, in connect
    return await connect_attempt
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/core.py", line 1082, in _connect
    comm = await connect(
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/comm/core.py", line 331, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://127.0.0.1:44567 after 10 s
distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:43289
Traceback (most recent call last):
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/asyncio/tasks.py", line 465, in wait_for
    fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/comm/core.py", line 327, in connect
    await asyncio.wait_for(comm.write(local_info), time_left())
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/asyncio/tasks.py", line 467, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/worker.py", line 2949, in gather_dep
    response = await get_data_from_worker(
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/worker.py", line 4173, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/utils_comm.py", line 386, in retry_operation
    return await retry(
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/utils_comm.py", line 371, in retry
    return await coro()
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/worker.py", line 4150, in _get_data
    comm = await rpc.connect(worker)
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/core.py", line 1146, in connect
    return await connect_attempt
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/core.py", line 1082, in _connect
    comm = await connect(
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/comm/core.py", line 331, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://127.0.0.1:43289 after 10 s
distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:44567
Traceback (most recent call last):
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/asyncio/tasks.py", line 465, in wait_for
    fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/comm/core.py", line 327, in connect
    await asyncio.wait_for(comm.write(local_info), time_left())
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/asyncio/tasks.py", line 467, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/worker.py", line 2949, in gather_dep
    response = await get_data_from_worker(
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/worker.py", line 4173, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/utils_comm.py", line 386, in retry_operation
    return await retry(
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/utils_comm.py", line 371, in retry
    return await coro()
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/worker.py", line 4150, in _get_data
    comm = await rpc.connect(worker)
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/core.py", line 1146, in connect
    return await connect_attempt
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/core.py", line 1082, in _connect
    comm = await connect(
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/comm/core.py", line 331, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://127.0.0.1:44567 after 10 s
/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:43289
Traceback (most recent call last):
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/asyncio/tasks.py", line 465, in wait_for
    fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/comm/core.py", line 327, in connect
    await asyncio.wait_for(comm.write(local_info), time_left())
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/asyncio/tasks.py", line 467, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/worker.py", line 2949, in gather_dep
    response = await get_data_from_worker(
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/worker.py", line 4173, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/utils_comm.py", line 386, in retry_operation
    return await retry(
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/utils_comm.py", line 371, in retry
    return await coro()
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/worker.py", line 4150, in _get_data
    comm = await rpc.connect(worker)
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/core.py", line 1146, in connect
    return await connect_attempt
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/core.py", line 1082, in _connect
    comm = await connect(
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/comm/core.py", line 331, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://127.0.0.1:43289 after 10 s
distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:44567
Traceback (most recent call last):
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/asyncio/tasks.py", line 465, in wait_for
    fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/comm/core.py", line 327, in connect
    await asyncio.wait_for(comm.write(local_info), time_left())
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/asyncio/tasks.py", line 467, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/worker.py", line 2949, in gather_dep
    response = await get_data_from_worker(
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/worker.py", line 4173, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/utils_comm.py", line 386, in retry_operation
    return await retry(
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/utils_comm.py", line 371, in retry
    return await coro()
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/worker.py", line 4150, in _get_data
    comm = await rpc.connect(worker)
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/core.py", line 1146, in connect
    return await connect_attempt
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/core.py", line 1082, in _connect
    comm = await connect(
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/comm/core.py", line 331, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://127.0.0.1:44567 after 10 s
distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:43289
Traceback (most recent call last):
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/asyncio/tasks.py", line 465, in wait_for
    fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/comm/core.py", line 327, in connect
    await asyncio.wait_for(comm.write(local_info), time_left())
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/asyncio/tasks.py", line 467, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/worker.py", line 2949, in gather_dep
    response = await get_data_from_worker(
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/worker.py", line 4173, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/utils_comm.py", line 386, in retry_operation
    return await retry(
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/utils_comm.py", line 371, in retry
    return await coro()
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/worker.py", line 4150, in _get_data
    comm = await rpc.connect(worker)
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/core.py", line 1146, in connect
    return await connect_attempt
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/core.py", line 1082, in _connect
    comm = await connect(
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/comm/core.py", line 331, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://127.0.0.1:43289 after 10 s
distributed.worker - ERROR - Worker stream died during communication: tcp://127.0.0.1:44567
Traceback (most recent call last):
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/asyncio/tasks.py", line 465, in wait_for
    fut.result()
asyncio.exceptions.CancelledError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/comm/core.py", line 327, in connect
    await asyncio.wait_for(comm.write(local_info), time_left())
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/asyncio/tasks.py", line 467, in wait_for
    raise exceptions.TimeoutError() from exc
asyncio.exceptions.TimeoutError

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/worker.py", line 2949, in gather_dep
    response = await get_data_from_worker(
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/worker.py", line 4173, in get_data_from_worker
    return await retry_operation(_get_data, operation="get_data_from_worker")
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/utils_comm.py", line 386, in retry_operation
    return await retry(
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/utils_comm.py", line 371, in retry
    return await coro()
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/worker.py", line 4150, in _get_data
    comm = await rpc.connect(worker)
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/core.py", line 1146, in connect
    return await connect_attempt
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/core.py", line 1082, in _connect
    comm = await connect(
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/distributed/comm/core.py", line 331, in connect
    raise OSError(
OSError: Timed out during handshake while connecting to tcp://127.0.0.1:44567 after 10 s
/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
[I 2022-06-27 12:12:46.819 ServerApp] Saving file at /m2_2022/sandbox/diag_std_ind.ipynb
[I 2022-06-27 12:12:49.417 ServerApp] Kernel restarted: 6b4a0f21-8857-420c-aa08-a8a68a57fc60
readline: /etc/inputrc: line 19: term: unknown variable name
[I 2022-06-27 12:14:17.810 ServerApp] Saving file at /m2_2022/m2lib22/stress_to_windterm.py
/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
[I 2022-06-27 12:14:40.982 ServerApp] Kernel restarted: 6b4a0f21-8857-420c-aa08-a8a68a57fc60
readline: /etc/inputrc: line 19: term: unknown variable name
[I 2022-06-27 12:14:47.405 ServerApp] Saving file at /m2_2022/sandbox/diag_std_ind.ipynb
[I 2022-06-27 12:16:17.114 ServerApp] Saving file at /m2_2022/m2lib22/stress_to_windterm.py
[I 2022-06-27 12:16:25.164 ServerApp] Kernel restarted: 6b4a0f21-8857-420c-aa08-a8a68a57fc60
Fatal Python error: init_import_size: Failed to import the site module
Python runtime state: initialized
Traceback (most recent call last):
  File "<frozen importlib._bootstrap_external>", line 148, in _path_is_mode_type
  File "<frozen importlib._bootstrap_external>", line 142, in _path_stat
FileNotFoundError: [Errno 2] No such file or directory: '/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site-packages/google/cloud/__init__.cpython-38-x86_64-linux-gnu.so'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site.py", line 580, in <module>
    main()
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site.py", line 567, in main
    known_paths = addsitepackages(known_paths)
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site.py", line 350, in addsitepackages
    addsitedir(sitedir, known_paths)
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site.py", line 208, in addsitedir
    addpackage(sitedir, name, known_paths)
  File "/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/site.py", line 169, in addpackage
    exec(line)
  File "<string>", line 1, in <module>
  File "<frozen importlib._bootstrap_external>", line 1407, in find_spec
  File "<frozen importlib._bootstrap_external>", line 1379, in _get_spec
  File "<frozen importlib._bootstrap_external>", line 1525, in find_spec
  File "<frozen importlib._bootstrap_external>", line 156, in _path_isfile
  File "<frozen importlib._bootstrap_external>", line 148, in _path_is_mode_type
KeyboardInterrupt
[I 2022-06-27 12:16:25.460 ServerApp] Kernel restarted: 6b4a0f21-8857-420c-aa08-a8a68a57fc60
readline: /etc/inputrc: line 19: term: unknown variable name
[I 2022-06-27 12:16:47.586 ServerApp] Saving file at /m2_2022/sandbox/diag_std_ind.ipynb
[I 2022-06-27 12:18:49.132 ServerApp] Saving file at /m2_2022/sandbox/diag_std_ind.ipynb
/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker_memory - WARNING - Worker is at 85% memory usage. Pausing worker.  Process memory: 10.63 GiB -- Worker memory limit: 12.50 GiB
distributed.worker_memory - WARNING - Worker is at 84% memory usage. Resuming worker. Process memory: 10.61 GiB -- Worker memory limit: 12.50 GiB
/home1/datahome/mdemol/.miniconda3/envs/m2env/lib/python3.8/contextlib.py:120: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
distributed.worker_memory - WARNING - Worker is at 85% memory usage. Pausing worker.  Process memory: 10.63 GiB -- Worker memory limit: 12.50 GiB
distributed.worker_memory - WARNING - Worker is at 85% memory usage. Pausing worker.  Process memory: 10.64 GiB -- Worker memory limit: 12.50 GiB
distributed.worker_memory - WARNING - Worker is at 85% memory usage. Pausing worker.  Process memory: 10.64 GiB -- Worker memory limit: 12.50 GiB
distributed.worker_memory - WARNING - Worker is at 84% memory usage. Resuming worker. Process memory: 10.57 GiB -- Worker memory limit: 12.50 GiB
distributed.worker_memory - WARNING - Worker is at 84% memory usage. Resuming worker. Process memory: 10.62 GiB -- Worker memory limit: 12.50 GiB
distributed.worker_memory - WARNING - Worker is at 84% memory usage. Resuming worker. Process memory: 10.62 GiB -- Worker memory limit: 12.50 GiB
Terminated
[C 2022-06-27 12:20:08.993 ServerApp] received signal 15, stopping
[C 2022-06-27 12:20:19.185 ServerApp] received signal 15, stopping
[I 2022-06-27 12:20:53.648 ServerApp] Shutting down 3 extensions
[I 2022-06-27 12:20:53.860 ServerApp] Shutting down 3 extensions
[I 2022-06-27 12:20:53.862 ServerApp] Shutting down 7 kernels
[I 2022-06-27 12:20:53.863 ServerApp] Shutting down 7 kernels
[I 2022-06-27 12:20:53.863 ServerApp] Kernel shutdown: 6b4a0f21-8857-420c-aa08-a8a68a57fc60
[I 2022-06-27 12:20:53.863 ServerApp] Kernel shutdown: d148c5ff-bd99-4645-a01c-068f0b87a957
[I 2022-06-27 12:20:53.863 ServerApp] Kernel shutdown: 4bb49fd4-7fa7-4b7e-95d4-35d8ae29d887
[I 2022-06-27 12:20:53.864 ServerApp] Kernel shutdown: 15e070c9-ce07-4d5e-98cb-4850a72e84f5
[I 2022-06-27 12:20:53.864 ServerApp] Kernel shutdown: e5c4908a-ae6c-490e-aa89-bc05b760b665
[I 2022-06-27 12:20:53.864 ServerApp] Kernel shutdown: 70a62b0f-7d51-4e5e-aaf3-645400b3fabf
[I 2022-06-27 12:20:53.864 ServerApp] Kernel shutdown: 9ba44625-cddc-4d31-8a94-b19fcb3506f2
[I 2022-06-27 12:20:53.864 ServerApp] Kernel shutdown: 6b4a0f21-8857-420c-aa08-a8a68a57fc60
[I 2022-06-27 12:20:53.864 ServerApp] Kernel shutdown: d148c5ff-bd99-4645-a01c-068f0b87a957
[I 2022-06-27 12:20:53.864 ServerApp] Kernel shutdown: 4bb49fd4-7fa7-4b7e-95d4-35d8ae29d887
[I 2022-06-27 12:20:53.864 ServerApp] Kernel shutdown: 15e070c9-ce07-4d5e-98cb-4850a72e84f5
[I 2022-06-27 12:20:53.865 ServerApp] Kernel shutdown: e5c4908a-ae6c-490e-aa89-bc05b760b665
[I 2022-06-27 12:20:53.865 ServerApp] Kernel shutdown: 70a62b0f-7d51-4e5e-aaf3-645400b3fabf
[I 2022-06-27 12:20:53.865 ServerApp] Kernel shutdown: 9ba44625-cddc-4d31-8a94-b19fcb3506f2
[I 2022-06-27 12:20:56.671 ServerApp] Shutting down 0 terminals
[I 2022-06-27 12:20:56.671 ServerApp] Shutting down 0 terminals
